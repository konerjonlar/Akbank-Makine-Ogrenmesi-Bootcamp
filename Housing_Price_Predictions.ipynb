{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/konerjonlar/Akbank-Makine-Ogrenmesi-Bootcamp/blob/main/Housing_Price_Predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Required Libraries"
      ],
      "metadata": {
        "id": "bJpqjq9Ph4_8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s5EC76W9NiOC"
      },
      "outputs": [],
      "source": [
        "# for data analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# for z-score\n",
        "from scipy import stats\n",
        "# for modellling and evaluating performance of the model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import Lasso, LinearRegression, Ridge, ElasticNet\n",
        "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Definition"
      ],
      "metadata": {
        "id": "qkO2enApiUWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, we need to load the Melbourne Housing dataset into our project. The\n",
        "quality and amount of data we collect will determine how good our predictive model\n",
        "can be. For this reason, we need to examine the dataset very carefully. We will\n",
        "estimate the price of a house using the Melbourne Housing dataset, which is a\n",
        "real-life example. Before evaluating any cost, we will start by analyzing the data\n",
        "using preprocessing techniques. We will then build our models and measure their\n",
        "performance to complete the project."
      ],
      "metadata": {
        "id": "80bF83ZCicPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gathering and Observing Data"
      ],
      "metadata": {
        "id": "PipSlNFtiLMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset to the project with the help of read_csv() and observe the first 5 columns"
      ],
      "metadata": {
        "id": "JpdxoXBIbRWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv('Melbourne_housing_FULL.csv')\n",
        "\n",
        "# Observe the 5 columns\n",
        "df.iloc[:, :5].head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "MHw38m6dRP27",
        "outputId": "247454e1-7430-4ebd-d6d0-ca3599a6a268"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-70f50d224f4a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Melbourne_housing_FULL.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Observe the 5 columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Melbourne_housing_FULL.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find the shape, number of columns and size of the dataset"
      ],
      "metadata": {
        "id": "CIeTuaY9bd_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the shape, number of columns and size of the dataset\n",
        "print(\"\\nShape of the Dataset:\\n\", df.shape,\n",
        "\"\\nNumber of Columns:\\n\", len(df.columns),\n",
        "\"\\nSize of the Dataset:\\n\", df.size)"
      ],
      "metadata": {
        "id": "V912kklobcFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Show the information of the dataset, which contains the number of columns, column labels, column data types, memory usage, range index, and the number of cells in each column (non-null values)."
      ],
      "metadata": {
        "id": "1NxfKRXtbmew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display general information about the dataset\n",
        "print(\"\\nColumn Labels:\", df.columns.tolist(),\n",
        "      \"\\nColumn Data Types:\", df.dtypes,\n",
        "      \"\\nMemory Usage:\", df.memory_usage(),\n",
        "      \"\\nRange Index:\", df.index,\n",
        "      \"\\nNumber of cells in each column (non-null values):\", df.count(), sep=\"\\n\")"
      ],
      "metadata": {
        "id": "Hm-vPgA8blzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "Q-1Wfx9ol-fB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examine the descriptive statistics of dataset"
      ],
      "metadata": {
        "id": "_HF26oc7b3WC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "aqvAcNB9cV_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The values of some variables are given as objects. At the same time, we observe that there are also categorical values. This might give us trouble when examining the dataset. Therefore, in such cases, we need to define the variables categorically."
      ],
      "metadata": {
        "id": "XHlCkZBjcHdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert these columns to categorical data type\n",
        "categorical_columns = [col for col in df.columns if df[col].dtype == 'object']"
      ],
      "metadata": {
        "id": "p4ZYtkOml8Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check for duplicate data. If there are duplicate data, clear them from the dataset."
      ],
      "metadata": {
        "id": "w1QK7hy2di6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for and remove duplicate data\n",
        "duplicates = df.duplicated()  # Find duplicate rows\n",
        "df_no_duplicates = df[~duplicates]  # Create a new DataFrame without duplicates\n",
        "\n",
        "# Print the number of duplicate rows\n",
        "print(\"Number of duplicate rows:\", duplicates.sum())\n",
        "\n",
        "# Update the original DataFrame to contain only the non-duplicate data\n",
        "df = df_no_duplicates"
      ],
      "metadata": {
        "id": "mLpi3L7sdJSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clear outlier data in the dataset. When you examine the dataset, you will observe that the outlier data is generally in the \"Landsize\" and \"Buildingarea\" variables."
      ],
      "metadata": {
        "id": "3jM8cGGMdjmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate z-scores for 'Landsize' and 'BuildingArea'\n",
        "z_scores_landsize = np.abs(stats.zscore(df['Landsize']))\n",
        "z_scores_building_area = np.abs(stats.zscore(df['BuildingArea']))\n",
        "\n",
        "# Define a z-score threshold for identifying outliers\n",
        "z_score_threshold = 3\n",
        "\n",
        "# Create boolean masks to identify outlier rows\n",
        "outliers_landsize = z_scores_landsize > z_score_threshold\n",
        "outliers_building_area = z_scores_building_area > z_score_threshold\n",
        "\n",
        "# Remove rows with outliers\n",
        "df = df[~(outliers_landsize | outliers_building_area)]"
      ],
      "metadata": {
        "id": "zqAY2YL8db2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find and remove the missing values on the dataset"
      ],
      "metadata": {
        "id": "jei8FRqndky-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find and remove rows with missing values\n",
        "df = df.dropna(subset=['Bathroom', 'Car'])\n",
        "\n",
        "# Fill missing values with the mode\n",
        "df['Bathroom'] = df['Bathroom'].fillna(df['Bathroom'].mode().idxmax())\n",
        "df['Car'] = df['Car'].fillna(df['Car'].mode().idxmax())\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "pBczwRb6dgBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Visualization"
      ],
      "metadata": {
        "id": "HfgTJlLhPRoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build a Histogram to visualize price distribution"
      ],
      "metadata": {
        "id": "hjzigxCkUbCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a histogram for the 'Price' variable\n",
        "plt.figure(figsize=(10, 6))  # Set the figure size\n",
        "plt.hist(df['Price'], bins=30, color='skyblue', edgecolor='black')  # Plot the histogram\n",
        "plt.title('Price Distribution')  # Set the title\n",
        "plt.xlabel('Price')  # Label the x-axis\n",
        "plt.ylabel('Frequency')  # Label the y-axis\n",
        "\n",
        "# Show the histogram\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5nOP1vhAQRUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Draw a pair plot to see the relationship between all numerical variables and the price variable"
      ],
      "metadata": {
        "id": "D7OAEZ0VUxny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the relevant numerical columns for the pair plot\n",
        "numerical_columns = ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Price']\n",
        "\n",
        "# Create a pair plot\n",
        "sns.pairplot(df[numerical_columns])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8Fv69YwoPk0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Draw a correlation matrix by using a heatmap on seaborn"
      ],
      "metadata": {
        "id": "kpTHKW-_U9F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix\n",
        "df_numeric = df.select_dtypes(include='number')\n",
        "correlation_matrix = df_numeric.corr()\n",
        "\n",
        "# Convert the correlation matrix to a NumPy array\n",
        "correlation_matrix_array = correlation_matrix.to_numpy()\n",
        "\n",
        "# Create a heatmap to visualize the correlation matrix\n",
        "plt.figure(figsize=(10, 8))  # Set the figure size\n",
        "sns.heatmap(correlation_matrix_array, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix Heatmap')  # Set the title\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rNNg8JxSP967"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement Label Encoder and One Hot encoder for categorical variables"
      ],
      "metadata": {
        "id": "k0zDF9duVDt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Apply Label Encoding for ordinal variables\n",
        "for column in categorical_columns:\n",
        "    if df[column].nunique() <= 5:\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "# Apply One-Hot Encoding for nominal variables\n",
        "df = pd.get_dummies(df, columns=[col for col in categorical_columns if df[col].nunique() > 5], drop_first=True)"
      ],
      "metadata": {
        "id": "bOngDcQzQikM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Selection"
      ],
      "metadata": {
        "id": "BGcvSX6cbDCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Since we are going to make a price estimation, we need to determine our x and y variables correctly."
      ],
      "metadata": {
        "id": "qoMC3M-1g0qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting our data into train-test in order to increase the performance of model training"
      ],
      "metadata": {
        "id": "dfs_XPXEg6Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(\"Price\", axis=1)\n",
        "y = df[\"Price\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.25)"
      ],
      "metadata": {
        "id": "pcUZdD2CteZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "'Lasso': Lasso(),\n",
        "'LinearRegression': LinearRegression(),\n",
        "'Ridge': Ridge(),\n",
        "'ElasticNet': ElasticNet(),\n",
        "'KNeighborsRegressor': KNeighborsRegressor(),\n",
        "'RandomForestRegressor': RandomForestRegressor(),\n",
        "'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
        "'AdaBoostRegressor': AdaBoostRegressor(n_estimators = 10, learning_rate = 1, loss = 'square', random_state = 2)\n",
        "        }"
      ],
      "metadata": {
        "id": "SmNw-ObGtH2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    predict = model.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, predict)\n",
        "    mse = mean_squared_error(y_test, predict)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, predict)\n",
        "    print(f\"##########{model_name}##########\")\n",
        "    print(mae)\n",
        "    print(mse)\n",
        "    print(rmse)\n",
        "    print(r2)"
      ],
      "metadata": {
        "id": "o5NCpjqmtQuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "OXThyDN-ghH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing models in each other"
      ],
      "metadata": {
        "id": "x9jI3kuZnpJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choose the best performing model by using evaluation metrics(MAE, MSE, RMSE, R2)"
      ],
      "metadata": {
        "id": "i9mpyPEwntWk"
      }
    }
  ]
}